apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
    name: __LWS_NAME__
spec:
    replicas: 1
    leaderWorkerTemplate:
        size: 2
        restartPolicy: RecreateGroupOnPodRestart

        workerTemplate:
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: gpu.nvidia.com/model
                            operator: In
                            values:
                              - H200

              containers:
              - name: vllm-worker
                image: "quay.io/tms/vllm-dev-base:0.0.17"
                imagePullPolicy: Always
                workingDir: /app
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    sleep infinity
                    #################
                    # Install vLLM
                    #################
                    VLLM_USE_PRECOMPILED=1 /init-scripts/init-vllm.sh
                    #################
                    # RUN vLLM
                    #################
                    START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                    if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                      #################
                      # Leader-only launch
                      #################
                      exec /app/venv/bin/vllm serve \
                        __MODEL__ \
                        --port 8080 \
                        --disable-log-requests \
                        --enable-expert-parallel \
                        --tensor-parallel-size $TP_SIZE \
                        --data-parallel-size $DP_SIZE \
                        --data-parallel-size-local $DP_SIZE_LOCAL \
                        --data-parallel-address $(LWS_LEADER_ADDRESS) \
                        --data-parallel-rpc-port 5555 \
                        --data-parallel-start-rank $START_RANK \
                        --trust-remote-code \
                        --kv-transfer-config \
                          '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                        --enforce-eager
                    else
                      #################
                      # Worker-only launch
                      #################
                      exec /app/venv/bin/vllm serve \
                        __MODEL__ \
                        --port 8080 \
                        --disable-log-requests \
                        --enable-expert-parallel \
                        --tensor-parallel-size $TP_SIZE \
                        --data-parallel-size $DP_SIZE \
                        --data-parallel-size-local $DP_SIZE_LOCAL \
                        --data-parallel-address $(LWS_LEADER_ADDRESS) \
                        --data-parallel-rpc-port 5555 \
                        --data-parallel-start-rank $START_RANK \
                        --trust-remote-code \
                        --kv-transfer-config \
                          '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                        --enforce-eager \
                        --headless
                    fi
                env:
                  - name: DP_SIZE
                    value: "4"
                  - name: TP_SIZE
                    value: "2"
                  - name: DP_SIZE_LOCAL
                    value: "2"
                  - name: VLLM_REPO_URL
                    value: "https://github.com/vllm-project/vllm.git"
                  - name: VLLM_BRANCH
                    value: "pplx_intranode"
                  - name: VLLM_ALL2ALL_BACKEND
#                    value: "naive"
                    value: "pplx"
#                    value: "deepep_high_throughput"
#                    value: "deepep_low_latency"
#
                    # Needed for GDRCOPY to be used.
                    # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                  - name: NVIDIA_GDRCOPY
                    value: "enabled"
#                  - name: NVIDIA_NVSWITCH
#                    value: "enabled"
#                  - name: NVIDIA_GDS
#                    value: "enabled"

                   # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
                  - name: NVIDIA_MOFED
                    value: "enabled"
#
                  - name: NCCL_DEBUG
                    value: "INFO"
                  - name: NVSHMEM_DEBUG
                    value: "TRACE"
                  - name: NVSHMEM_DEBUG_SUBSYS
                    value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                  - name: NVSHMEM_REMOTE_TRANSPORT
                    value: "ibrc"
                  - name: NVSHMEM_IB_ENABLE_IBGDA
                    value: "true"
                  - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                    value: "true"
                  - name: NVSHMEM_HCA_LIST
                    value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                  - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                    value: "eth0"
                  - name: GLOO_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_IB_HCA
                    value: "ibp"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                  - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                    value: "6555"
                  - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                    valueFrom:
                      fieldRef:
                        fieldPath: status.podIP

                securityContext:
                  capabilities:
                    add: [ "IPC_LOCK" ]
                resources:
                  limits:
                    nvidia.com/gpu: "4"
                    memory: 64Gi
                    ephemeral-storage: 256Gi
                    rdma/ib: 1
                  requests:
                    cpu: 8
                    memory: 64Gi
                    ephemeral-storage: 256Gi
                    nvidia.com/gpu: "4"
                    rdma/ib: 1
                volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
                  - name: init-scripts-volume
                    mountPath: /init-scripts
              volumes:
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                # Needed for NCCL to function
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi

---
apiVersion: v1
kind: Service
metadata:
    name: __SERVICE_NAME__
spec:
    ports:
        - name: http
          port: 8080
          protocol: TCP
          targetPort: 8080
    selector:
        leaderworkerset.sigs.k8s.io/name: __LWS_NAME__
        leaderworkerset.sigs.k8s.io/worker-index: "0"
    type: ClusterIP
